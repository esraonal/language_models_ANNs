{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65e6d4e2",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Install `tf-nightly` via `pip install tf-nightly`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d3c1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install tf-nightly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af4ac25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "from dataclasses import dataclass\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import pickle\n",
    "import glob\n",
    "import re\n",
    "from pprint import pprint\n",
    "import apache_beam\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b17f94f-2970-4288-b986-be215aa1085a",
   "metadata": {},
   "source": [
    "## Test Tensorflow and GPU use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ff9f5f-5618-4cb4-a86d-f339922a4c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb17854",
   "metadata": {},
   "source": [
    "## Set-up Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5059b1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    MAX_LEN = 64\n",
    "    BATCH_SIZE = 32\n",
    "    LR = 0.001\n",
    "    VOCAB_SIZE = 30000\n",
    "    EMBED_DIM = 128\n",
    "    NUM_HEAD = 8  # used in bert model\n",
    "    FF_DIM = 128  # used in bert model\n",
    "    NUM_LAYERS = 1\n",
    "\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f107b5",
   "metadata": {},
   "source": [
    "## <span style=\"color: #333399;\">Load the Data</span>\n",
    "\n",
    "- **Data Loading**: We will first load the Amazon Polarity dataset and separate it into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c239da3-2239-4eea-a987-081c872d2973",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"amazon_polarity\")\n",
    "# print(dataset)\n",
    "\n",
    "# Calculate lengths\n",
    "length_train = len(dataset['train'])\n",
    "length_test = len(dataset['test'])\n",
    "\n",
    "# Use list comprehensions to create the lists\n",
    "train_amazonreview = [dataset['train'][i]['content'] for i in range(length_train)]\n",
    "test_amazonreview = [dataset['test'][i]['content'] for i in range(length_test)]\n",
    "\n",
    "# Print the results\n",
    "print(length_train)\n",
    "print(length_test)\n",
    "# print(train_amazonreview[:10])\n",
    "\n",
    "# Access a specific test sentence\n",
    "test_sentence = dataset['test'][100]['content']\n",
    "# print(test_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9127e6c-0ed7-4b34-9b9f-b6e734e33e05",
   "metadata": {},
   "source": [
    "## <span style=\"color: #333399;\">Sample Data for Training</span>\n",
    "\n",
    "### <span style=\"color: #333399;\">Current Status</span>\n",
    "\n",
    "- **Training Data Utilization**: 100,000 out of 3,600,000 samples have been used for training purposes so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "10c4fd7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample from the list\n",
    "amazon_sample = random.sample(train_amazonreview, 100000)\n",
    "amazon_test_sample = random.sample(test_amazonreview, 50)\n",
    "\n",
    "# Convert the samples to numpy arrays if needed\n",
    "arr_amazon_sample = np.array(amazon_sample)\n",
    "arr_amazon_test_sample = np.array(amazon_test_sample)\n",
    "\n",
    "# Print the sampled test reviews\n",
    "# print(arr_amazon_test_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a635ba37-7f43-4a7d-af7e-17f423768f25",
   "metadata": {},
   "source": [
    "## <span style=\"color: #333399;\">Overview of Text Processing and Vectorization</span>\n",
    "\n",
    "### <span style=\"color: #333399;\">General Functionality</span>\n",
    "\n",
    "- **Text Preprocessing**: Normalizes input text by lowercasing and cleaning up special characters, preparing it for further processing.\n",
    "- **Text Vectorization**: Converts text into a sequence of integers where each integer represents a unique token in the vocabulary. This transformation is essential for feeding textual data into machine learning models.\n",
    "\n",
    "### <span style=\"color: #333399;\">Customization Details</span>\n",
    "\n",
    "- **Custom Preprocessing Function**: Uses a specialized function to preprocess text data by specifically targeting sentence-ending punctuations and incorporating a unique <span style=\"color: #0077CC;\">**[sep]**</span> token to clearly demarcate sentence boundaries.\n",
    "- **Adjustable Vocabulary**: The vectorization setup includes a mechanism to adapt the vocabulary based on the input texts and extend it by incorporating special tokens like <span style=\"color: #CC0077;\">**[mask]**</span>, which are crucial for tasks such as masked language modeling.\n",
    "- **Devectorization Method**: Provides a method to reverse the vectorization process, which is helpful for verifying the correctness of the vectorization process and understanding model predictions.\n",
    "\n",
    "These customizations enhance the model's ability to handle various textual nuances, making it suitable for advanced NLP tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7176b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(input_data):\n",
    "    lowercase = tf.strings.lower(input_data)\n",
    "    \n",
    "    # Replace multiple sentence-ending punctuations with [sep]\n",
    "    sentence_sep_pattern = r\"([!?.]+)\"\n",
    "    standardized_text = tf.strings.regex_replace(lowercase, sentence_sep_pattern, \" [sep] \")\n",
    "\n",
    "    # Remove other special characters except the [sep] token\n",
    "    stripped_html = tf.strings.regex_replace(standardized_text, \"<br />\", \" \")\n",
    "    cleaned_text = tf.strings.regex_replace(\n",
    "        stripped_html, r\"[{}]+\".format(re.escape(\"#$%&'()*+,-/:;<=>@^_`{|}~\\\"\")), \"\"\n",
    "    )\n",
    "    \n",
    "    # Ensure there is no trailing [sep] token at the end\n",
    "    cleaned_text = tf.strings.regex_replace(cleaned_text, r\"\\s*\\[sep\\]\\s*$\", \"\")\n",
    "    \n",
    "    return cleaned_text\n",
    "\n",
    "def get_vectorize_layer(texts, vocab_size, max_seq, special_tokens=[\"[MASK]\"]):\n",
    "\n",
    "    # Initialize TextVectorization layer with custom standardization\n",
    "    vectorize_layer = TextVectorization(\n",
    "        max_tokens=vocab_size,\n",
    "        output_mode=\"int\",\n",
    "        standardize=preprocess_text,\n",
    "        output_sequence_length=max_seq,\n",
    "    )\n",
    "\n",
    "    # Adapt the vectorize layer to the input texts\n",
    "    vectorize_layer.adapt(texts)\n",
    "\n",
    "    # Get the current vocabulary\n",
    "    vocab = vectorize_layer.get_vocabulary()\n",
    "\n",
    "    vocab = vocab[:-1] + special_tokens\n",
    "\n",
    "    # Reapply the updated vocabulary to the vectorization layer\n",
    "    vectorize_layer.set_vocabulary(vocab)\n",
    "\n",
    "    return vectorize_layer\n",
    "\n",
    "def devectorize_sequences(vectorize_layer, sequences):\n",
    "    # Get the vocabulary and reverse map from indices to tokens\n",
    "    vocab = vectorize_layer.get_vocabulary()\n",
    "    index_to_token = {i: token for i, token in enumerate(vocab)}\n",
    "\n",
    "    # Convert integer sequences back to text\n",
    "    def index_to_text(indices):\n",
    "        return ' '.join(index_to_token.get(index, '') for index in indices)\n",
    "    \n",
    "    # Apply the conversion to all sequences\n",
    "    return [index_to_text(sequence) for sequence in sequences]\n",
    "\n",
    "# Configure the vectorize layer\n",
    "vectorize_layer = get_vectorize_layer(\n",
    "    train_amazonreview,\n",
    "    config.VOCAB_SIZE,\n",
    "    config.MAX_LEN,\n",
    "    special_tokens=[\"[mask]\"],\n",
    ")\n",
    "\n",
    "# Get mask token id for masked language model\n",
    "mask_token_id = vectorize_layer([\"[mask]\"]).numpy()[0][0]\n",
    "print(\"\\nMask token ID:\", mask_token_id)\n",
    "\n",
    "# print(vectorize_layer.get_vocabulary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442056fc-0299-42fa-8e38-b6dd85771f1b",
   "metadata": {},
   "source": [
    "## <span style=\"color: #333399;\">Save Text Vectorization Configuration</span>\n",
    "\n",
    "- **Function Overview**: This function saves the configuration and vocabulary of a TensorFlow `TextVectorization` layer to ensure model consistency during deployment or further training sessions.\n",
    "\n",
    "### <span style=\"color: #333399;\">Key Processes:</span>\n",
    "- **Vocabulary Saving**: The vocabulary of the `TextVectorization` layer, crucial for encoding text data, is serialized and saved using Python's `pickle` module.\n",
    "- **Configuration Saving**: Additional configuration parameters like maximum token counts and output sequence lengths are also serialized and saved. This step is essential for recreating the vectorization environment accurately in different computational contexts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "491defd1-3f9b-491b-a2d2-0c4b6d45fc59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming `vectorize_layer` is your TextVectorization layer\n",
    "def save_vectorize_layer(vectorize_layer, filename):\n",
    "    # Save the vocabulary\n",
    "    vocab = vectorize_layer.get_vocabulary()\n",
    "    with open(filename + '_vocab.pkl', 'wb') as f:\n",
    "        pickle.dump(vocab, f)\n",
    "\n",
    "    # Save other necessary parameters if needed\n",
    "    config_vec = {\n",
    "        'max_tokens': config.VOCAB_SIZE,\n",
    "        'output_sequence_length': config.MAX_LEN\n",
    "    }\n",
    "    with open(filename + '_config.pkl', 'wb') as f:\n",
    "        pickle.dump(config_vec, f)\n",
    "\n",
    "save_vectorize_layer(vectorize_layer, 'vectorize_layer')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0285afe7-755b-4755-9d58-237a1e055618",
   "metadata": {},
   "source": [
    "## <span style=\"color: #333399;\">Load Text Vectorization Configuration</span>\n",
    "\n",
    "- **Function Overview**: This function retrieves and reinstates the configuration and vocabulary of a TensorFlow `TextVectorization` layer from previously saved files, ensuring seamless continuation of text processing capabilities in different operational environments.\n",
    "\n",
    "### <span style=\"color: #333399;\">Key Processes:</span>\n",
    "- **Vocabulary Loading**: The vocabulary, crucial for the correct encoding of text data, is loaded from a pickle file. This step re-establishes the essential word-to-index mappings used in text vectorization.\n",
    "- **Configuration Loading**: Additional parameters such as maximum token counts and output sequence lengths are loaded from another pickle file. These settings are vital for maintaining the operational consistency of the vectorization process across different setups.\n",
    "- **Layer Recreation**: After loading the necessary configurations, the `TextVectorization` layer is recreated with the exact parameters that were previously in use, ensuring that the layer operates exactly as it did before saving.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a872f17b-cdf3-4c0e-9310-68152f9b7fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vectorize_layer(filename):\n",
    "    # Load the vocabulary\n",
    "    with open(filename + '_vocab.pkl', 'rb') as f:\n",
    "        vocab = pickle.load(f)\n",
    "\n",
    "    # Load other necessary parameters\n",
    "    with open(filename + '_config.pkl', 'rb') as f:\n",
    "        config_vec = pickle.load(f)\n",
    "\n",
    "    # Recreate the TextVectorization layer with the loaded parameters\n",
    "    vectorize_layer = TextVectorization(\n",
    "        max_tokens=config_vec['max_tokens'],\n",
    "        output_mode='int',\n",
    "        output_sequence_length=config_vec['output_sequence_length']\n",
    "    )\n",
    "\n",
    "    vectorize_layer.set_vocabulary(vocab)\n",
    "    return vectorize_layer\n",
    "\n",
    "vectorize_layer = load_vectorize_layer('vectorize_layer')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d07243-ec00-4222-b26f-104283f79f8f",
   "metadata": {},
   "source": [
    "## <span style=\"color: #333399;\">Left-Padding Sequences Function</span>\n",
    "\n",
    "- **Purpose**: Adjusts the alignment of token sequences by left-padding them, which is essential for certain types of data processing where alignment affects model performance.\n",
    "- **Process**:\n",
    "    - **Initialization**: Creates a new array of zeros with the same dimensions as the input, ensuring a clean slate for padding adjustments.\n",
    "    - **Non-zero Counting**: Determines the count of non-zero tokens (actual data) in each sequence to avoid padding these meaningful tokens.\n",
    "    - **Padding Calculation**: Computes the correct starting position for the non-zero tokens in the new array to shift them towards the end, ensuring the padding is on the left.\n",
    "    - **Data Rearrangement**: Moves the non-zero elements to their new positions, effectively left-padding the sequence with zeros.\n",
    "\n",
    "This function is crucial for preparing data in a format that is compatible with neural network architectures that require fixed-length input sequences.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e69a9642-d78c-4e9b-aeac-0868e80dd774",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to left-pad sequences\n",
    "def left_pad_sequences(vectorized_texts):\n",
    "    # Initialize a new array of zeros with the same shape as the input\n",
    "    padded_texts = np.zeros_like(vectorized_texts)\n",
    "    \n",
    "    # For each sequence in the input array\n",
    "    for i, seq in enumerate(vectorized_texts):\n",
    "        # Count the number of non-zero (non-padding) elements\n",
    "        non_zeros = np.count_nonzero(seq)\n",
    "        \n",
    "        # Compute the starting index for non-padding elements in the new array\n",
    "        start_index = len(seq) - non_zeros\n",
    "        \n",
    "        # Place the non-zero elements at the end of the corresponding new sequence\n",
    "        padded_texts[i, start_index:] = seq[:non_zeros]\n",
    "    \n",
    "    return padded_texts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a127f7be-0c59-453a-add7-a7b520015385",
   "metadata": {},
   "source": [
    "## <span style=\"color: #333399;\">Text Encoding and Token Masking Functions</span>\n",
    "\n",
    "### <span style=\"color: #333399;\">Text Encoding</span>\n",
    "- **Purpose**: Converts input texts into a vectorized format using a provided `TextVectorization` layer, facilitating machine learning processing.\n",
    "- **Process**: \n",
    "    - The input text is transformed into a sequence of integer tokens that represent each word based on a predefined vocabulary.\n",
    "    - The resulting tensor is then converted into a NumPy array, preparing it for further manipulation such as padding or masking.\n",
    "\n",
    "### <span style=\"color: #333399;\">Token Masking</span>\n",
    "- **Purpose**: Selectively masks tokens in the vectorized text to support tasks like training a masked language model (MLM), which can improve the model's ability to predict missing or masked words.\n",
    "- **Process**:\n",
    "    - **Initialization**: Masks 15% of the non-special, non-zero-padding tokens in each sequence. Special tokens and padding are excluded from masking to maintain contextual integrity and avoid altering structural elements.\n",
    "    - **Masking Logic**: Ensures that at least the last valid token in each sequence is masked, which helps the model learn to predict token endings in sentences.\n",
    "    - **Output Creation**: Generates three outputs:\n",
    "        - **Masked Text**: The input text with selected tokens replaced by a mask token ID.\n",
    "        - **Labels**: The original tokens at the masked positions, used as targets for model training.\n",
    "        - **Sample Weights**: Indicators (1s and 0s) for which tokens should be considered during the loss calculation, focusing model learning on the masked positions.\n",
    "\n",
    "### <span style=\"color: #333399;\">TensorFlow Dataset Preparation</span>\n",
    "- **Purpose**: Converts the masked text, labels, and sample weights into a TensorFlow dataset, suitable for training machine learning models in a batched and shuffled manner.\n",
    "- **Process**:\n",
    "    - The data is shuffled and batched according to a predefined batch size, making it ready for efficient training cycles.\n",
    "    - A sample batch is retrieved and printed, providing a snapshot of the data that will be fed into the model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "958a0e15-6457-4a46-b95c-c266a3e6821c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Masked Inputs:\n",
      " [[   22   399    45 ...     2    74 29999]\n",
      " [    0     0     0 ...     2   270 29999]\n",
      " [    5 29999 29999 ...   131     7 29999]\n",
      " ...\n",
      " [21533     1    49 ...  4251  1947 29999]\n",
      " [    0     0     0 ...    11     3 29999]\n",
      " [29999    21    83 ...  2527    18 29999]]\n",
      "Labels:\n",
      " [[ -1  -1  -1 ...  -1  -1  75]\n",
      " [ -1  -1  -1 ...  -1  -1 499]\n",
      " [ -1  94  36 ...  -1  -1 730]\n",
      " ...\n",
      " [ -1  -1  -1 ...  -1  -1  14]\n",
      " [ -1  -1  -1 ...  -1  -1 110]\n",
      " [  5  -1  -1 ...  -1  -1   6]]\n",
      "Sample Weights:\n",
      " [[0 0 0 ... 0 0 1]\n",
      " [0 0 0 ... 0 0 1]\n",
      " [0 1 1 ... 0 0 1]\n",
      " ...\n",
      " [0 0 0 ... 0 0 1]\n",
      " [0 0 0 ... 0 0 1]\n",
      " [1 0 0 ... 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "# Encode function\n",
    "def encode(texts, vectorize_layer):\n",
    "    \"\"\"Encodes the input texts using the provided vectorization layer.\"\"\"\n",
    "    vectorized = vectorize_layer(texts)\n",
    "    \n",
    "    # Convert the vectorized text tensor to a NumPy array using .numpy()\n",
    "    vectorized_numpy = vectorized.numpy()\n",
    "    \n",
    "    # Apply the left-padding function to the NumPy array\n",
    "    left_padded = left_pad_sequences(vectorized_numpy)\n",
    "\n",
    "    return left_padded\n",
    "\n",
    "# Mask tokens function\n",
    "def mask_tokens(vectorized_text, mask_token_id=29999, special_token_ids=[0, 1, 2]):\n",
    "    \"\"\"Masks 15% of the tokens in the vectorized text, excluding special tokens, and includes the last token in the sequence in every sequence.\"\"\"\n",
    "    masked_text = np.copy(vectorized_text)\n",
    "    labels = -1 * np.ones_like(vectorized_text)  # Initialize labels with -1\n",
    "    sample_weights = np.zeros_like(vectorized_text)  # Initialize sample weights with 0\n",
    "\n",
    "    for i, sequence in enumerate(masked_text):\n",
    "        # Identify valid indices (not special tokens and not zero-padding)\n",
    "        valid_indices = [idx for idx, token in enumerate(sequence) if token not in special_token_ids and token != 0]\n",
    "        \n",
    "        # Continue only if there are any valid indices\n",
    "        if not valid_indices:\n",
    "            continue\n",
    "        \n",
    "        # Calculate the number of tokens to mask: 15% of valid tokens\n",
    "        num_to_mask = max(1, int(0.15 * len(valid_indices)))\n",
    "        \n",
    "        # Randomly choose indices to mask, ensuring not to include the last valid token yet\n",
    "        if len(valid_indices) > 1:  # Check to ensure there's more than one valid index to avoid selection error\n",
    "            mask_indices = np.random.choice(valid_indices[:-1], num_to_mask - 1, replace=False)\n",
    "        else:\n",
    "            mask_indices = np.array([], dtype=int)\n",
    "        \n",
    "        # Always include the last valid token in the mask indices\n",
    "        mask_indices = np.append(mask_indices, valid_indices[-1])\n",
    "\n",
    "        # Mask the chosen indices\n",
    "        masked_text[i, mask_indices] = mask_token_id\n",
    "        # Set labels for masked tokens (useful for training to predict these tokens)\n",
    "        labels[i, mask_indices] = vectorized_text[i, mask_indices]\n",
    "        # Set sample weights to 1 for masked tokens (indicating these should be learned)\n",
    "        sample_weights[i, mask_indices] = 1  \n",
    "\n",
    "    return masked_text, labels, sample_weights\n",
    "    \n",
    "# Vectorize the example texts\n",
    "vectorized_text = encode(amazon_sample, vectorize_layer)\n",
    "\n",
    "# Apply the masking function\n",
    "masked_text, labels, sample_weights = mask_tokens(vectorized_text)\n",
    "\n",
    "# Creating the TensorFlow dataset\n",
    "mlm_ds = tf.data.Dataset.from_tensor_slices((masked_text, labels, sample_weights))\n",
    "mlm_ds = mlm_ds.shuffle(1000).batch(Config.BATCH_SIZE)\n",
    "\n",
    "# Take one batch and print its contents\n",
    "for x_batch, y_batch, sw_batch in mlm_ds.take(1):\n",
    "    print(\"Masked Inputs:\\n\", x_batch.numpy())\n",
    "    print(\"Labels:\\n\", y_batch.numpy())\n",
    "    print(\"Sample Weights:\\n\", sw_batch.numpy())\n",
    "# Sample texts (fit the vectorize layer to these)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f099ffe-04d3-4e24-8768-8789821486e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "amazon_example = [\n",
    "    \"I got one for christmas and it is one of the worst cars i have had I have went through two motors the last one i got and ran it twice and it would not work so I called traxxas and they sent me a new one and they didn't say sorry or anything they acted like they didn't even care. they don't even want to make their customers happy I will not ever buy another traxxas product again just because of the rustler and their support when i called them. and the car is not all that great the battery charger takes eight hours to charge which is terrible because people don't want to wait eight hours to play with the car. they make the car out of very cheap materials and they want you to have to buy a faster charger because they know that most people are going to buy a faster charger.\",\n",
    "    \"I would give it 20 stars if I could. Absolutely fantastic storyline. Couldn't put my Kindle Fire down. Wish it would happen to me for real.\",\n",
    "    \"Nice little cooker, but the button broke in about 2 months. I took it apart to try to fix it and the button is held on by two tiny screws into little plastic holders that were broken and ruined. I'm not hamfisted, so not a very strong setup for the only moving button on the thing. Oh well, that's why it's $15. Makes me mad though because now it's in the landfill and I would have gladly paid a few extra bucks (or more like 25 cents) so it would last longer.\"\n",
    "]\n",
    "aa = encode(amazon_example, vectorize_layer)\n",
    "print(aa)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a1a2f7-5966-48c0-b751-cd31c60df685",
   "metadata": {},
   "source": [
    "## <span style=\"color: #333399;\">Position Embedding Layer Implementation</span>\n",
    "\n",
    "- **Purpose**: Implements a custom `PositionEmbeddingLayer` class to provide positional context to tokens in sequence data, enhancing the model's understanding of sequence order.\n",
    "- **Key Features**:\n",
    "  - **Initialization**: Sets up an embedding layer specifically for encoding positional information of tokens in a sequence, using predefined sequence length and output dimensions.\n",
    "  - **Position Indices**: Dynamically generates position indices for each token in a sequence and retrieves their embeddings.\n",
    "  - **Batch Handling**: Scales the position embeddings to match the batch size of the inputs, ensuring every sequence in the batch receives a corresponding position embedding.\n",
    "  - **Serialization Support**: Includes customization of the `get_config` method to ensure that the layer's configuration can be saved and loaded effectively.\n",
    "\n",
    "This layer is crucial for models that process sequence data where the order of elements is important, such as in natural language processing tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "49852e8d-a3d9-48cf-b53a-ad53f0f91010",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Layer, Embedding\n",
    "\n",
    "class PositionEmbeddingLayer(Layer):\n",
    "    def __init__(self, sequence_length, output_dim, **kwargs):\n",
    "        super(PositionEmbeddingLayer, self).__init__(**kwargs)\n",
    "        self.sequence_length = sequence_length\n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "        # Initialize the position embedding layer\n",
    "        self.position_embedding_layer = Embedding(\n",
    "            input_dim=sequence_length, \n",
    "            output_dim=output_dim,\n",
    "            name=\"position_embeddings\"\n",
    "        )\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        # Determine the batch size from the inputs tensor\n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "\n",
    "        # Generate position indices based on the provided sequence length\n",
    "        position_indices = tf.range(start=0, limit=self.sequence_length, delta=1)\n",
    "        \n",
    "        # Get embeddings for these position indices\n",
    "        embedded_positions = self.position_embedding_layer(position_indices)\n",
    "        \n",
    "        # Replicate the position embeddings for each instance in the batch\n",
    "        embedded_positions = tf.tile(tf.expand_dims(embedded_positions, 0), [batch_size, 1, 1])\n",
    "\n",
    "        return embedded_positions\n",
    "\n",
    "    # Override get_config to enable serialization of the custom layer\n",
    "    def get_config(self):\n",
    "        config = super(PositionEmbeddingLayer, self).get_config()\n",
    "        config.update({\n",
    "            \"sequence_length\": self.sequence_length,\n",
    "            \"output_dim\": self.output_dim,\n",
    "        })\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "542d4ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "def attention(query, key, value, layer_idx):\n",
    "    \"\"\"Multi-headed self-attention and feed-forward network for transformer models.\n",
    "\n",
    "    Args:\n",
    "        query: Input query for self-attention.\n",
    "        key: Input key for self-attention.\n",
    "        value: Input value for self-attention.\n",
    "        layer_idx (int): Layer index to name layers uniquely.\n",
    "        config: Configuration object containing model parameters.\n",
    "\n",
    "    Returns:\n",
    "        Tensor: Output tensor from the transformer encoder block.\n",
    "    \"\"\"\n",
    "    # Multi-headed self-attention layer\n",
    "    attention_layer = layers.MultiHeadAttention(\n",
    "        num_heads=config.NUM_HEAD,\n",
    "        key_dim=config.EMBED_DIM // config.NUM_HEAD,\n",
    "        name=f\"encoder_{layer_idx}/multiheadattention\"\n",
    "    )\n",
    "    attention_output = attention_layer(query, key, value)\n",
    "    attention_output = layers.Dropout(0.1, name=f\"encoder_{layer_idx}/att_dropout\")(attention_output)\n",
    "    attention_output = layers.LayerNormalization(\n",
    "        epsilon=1e-6, name=f\"encoder_{layer_idx}/att_layernormalization\"\n",
    "    )(query + attention_output)\n",
    "\n",
    "    # Position-wise feed-forward network\n",
    "    ffn = tf.keras.Sequential([\n",
    "        layers.Dense(config.FF_DIM, activation=\"relu\", name=f\"encoder_{layer_idx}/ffn_dense1\"),\n",
    "        layers.Dense(config.EMBED_DIM, name=f\"encoder_{layer_idx}/ffn_dense2\")\n",
    "    ], name=f\"encoder_{layer_idx}/ffn\")\n",
    "\n",
    "    ffn_output = ffn(attention_output)\n",
    "    ffn_output = layers.Dropout(0.1, name=f\"encoder_{layer_idx}/ffn_dropout\")(ffn_output)\n",
    "    sequence_output = layers.LayerNormalization(\n",
    "        epsilon=1e-6, name=f\"encoder_{layer_idx}/ffn_layernormalization\"\n",
    "    )(attention_output + ffn_output)\n",
    "\n",
    "    return sequence_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "3a8ec0e2-93f3-4f30-8733-c55d7eb74ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='auto')\n",
    "loss_tracker = keras.metrics.Mean(name=\"loss\")\n",
    "\n",
    "\n",
    "class MaskedLanguageModel(keras.Model):\n",
    "    def train_step(self, inputs):\n",
    "        if len(inputs) == 3:\n",
    "            features, labels, sample_weight = inputs\n",
    "        else:\n",
    "            features, labels = inputs\n",
    "            sample_weight = None\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = self(features, training=True)\n",
    "            loss = loss_fn(labels, predictions, sample_weight=sample_weight)\n",
    "\n",
    "        # Compute gradients\n",
    "        trainable_vars = self.trainable_variables\n",
    "        gradients = tape.gradient(loss, trainable_vars)\n",
    "\n",
    "        # Update weights\n",
    "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "\n",
    "        # Compute our own metrics\n",
    "        loss_tracker.update_state(loss, sample_weight=sample_weight)\n",
    "\n",
    "        # Return a dict mapping metric names to current value\n",
    "        return {\"loss\": loss_tracker.result()}\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        # We list our `Metric` objects here so that `reset_states()` can be\n",
    "        # called automatically at the start of each epoch\n",
    "        # or at the start of `evaluate()`.\n",
    "        # If you don't implement this property, you have to call\n",
    "        # `reset_states()` yourself at the time of your choosing.\n",
    "        return [loss_tracker]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "854e1cc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 64)\n",
      "(None, 64, 128)\n",
      "(None, 64, 128)\n",
      "(None, 64, 128)\n",
      "(None, 64, 128)\n",
      "(None, 64, 30000)\n",
      "Model: \"masked_bert_model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_ids (InputLayer)         [(None, 64)]         0           []                               \n",
      "                                                                                                  \n",
      " word_embedding (Embedding)     (None, 64, 128)      3840000     ['input_ids[0][0]']              \n",
      "                                                                                                  \n",
      " position_embedding (PositionEm  (None, 64, 128)     8192        ['input_ids[0][0]']              \n",
      " beddingLayer)                                                                                    \n",
      "                                                                                                  \n",
      " embeddings_add (Add)           (None, 64, 128)      0           ['word_embedding[0][0]',         \n",
      "                                                                  'position_embedding[0][0]']     \n",
      "                                                                                                  \n",
      " encoder_0/multiheadattention (  (None, 64, 128)     66048       ['embeddings_add[0][0]',         \n",
      " MultiHeadAttention)                                              'embeddings_add[0][0]',         \n",
      "                                                                  'embeddings_add[0][0]']         \n",
      "                                                                                                  \n",
      " encoder_0/att_dropout (Dropout  (None, 64, 128)     0           ['encoder_0/multiheadattention[0]\n",
      " )                                                               [0]']                            \n",
      "                                                                                                  \n",
      " tf.__operators__.add_8 (TFOpLa  (None, 64, 128)     0           ['embeddings_add[0][0]',         \n",
      " mbda)                                                            'encoder_0/att_dropout[0][0]']  \n",
      "                                                                                                  \n",
      " encoder_0/att_layernormalizati  (None, 64, 128)     256         ['tf.__operators__.add_8[0][0]'] \n",
      " on (LayerNormalization)                                                                          \n",
      "                                                                                                  \n",
      " encoder_0/ffn (Sequential)     (None, 64, 128)      33024       ['encoder_0/att_layernormalizatio\n",
      "                                                                 n[0][0]']                        \n",
      "                                                                                                  \n",
      " encoder_0/ffn_dropout (Dropout  (None, 64, 128)     0           ['encoder_0/ffn[0][0]']          \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.__operators__.add_9 (TFOpLa  (None, 64, 128)     0           ['encoder_0/att_layernormalizatio\n",
      " mbda)                                                           n[0][0]',                        \n",
      "                                                                  'encoder_0/ffn_dropout[0][0]']  \n",
      "                                                                                                  \n",
      " encoder_0/ffn_layernormalizati  (None, 64, 128)     256         ['tf.__operators__.add_9[0][0]'] \n",
      " on (LayerNormalization)                                                                          \n",
      "                                                                                                  \n",
      " mlm_cls (Dense)                (None, 64, 30000)    3870000     ['encoder_0/ffn_layernormalizatio\n",
      "                                                                 n[0][0]']                        \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 7,817,776\n",
      "Trainable params: 7,817,776\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def build_bert_encoder():\n",
    "    \"\"\"\n",
    "    Constructs a BERT-like masked language model encoder.\n",
    "\n",
    "    Args:\n",
    "        config: A configuration object containing attributes like MAX_LEN, VOCAB_SIZE, EMBED_DIM, NUM_LAYERS, LR.\n",
    "\n",
    "    Returns:\n",
    "        A compiled TensorFlow model ready for training.\n",
    "    \"\"\"\n",
    "    # Define the input layer\n",
    "    inputs = layers.Input(shape=(config.MAX_LEN,), dtype=tf.int64, name=\"input_ids\")\n",
    "\n",
    "    print(inputs.shape)\n",
    "    \n",
    "    # Embedding layers: token embeddings + positional embeddings\n",
    "    word_embeddings = layers.Embedding(\n",
    "        input_dim=config.VOCAB_SIZE, output_dim=config.EMBED_DIM, name=\"word_embedding\"\n",
    "    )(inputs)\n",
    "\n",
    "    print(word_embeddings.shape)\n",
    "    \n",
    "    position_embeddings = PositionEmbeddingLayer(\n",
    "        sequence_length=config.MAX_LEN, output_dim=config.EMBED_DIM, name=\"position_embedding\"\n",
    "    )(inputs)\n",
    "\n",
    "    print(position_embeddings.shape)\n",
    "    # Sum embeddings\n",
    "    embeddings = layers.Add(name=\"embeddings_add\")([word_embeddings, position_embeddings])\n",
    "\n",
    "    print(embeddings.shape)\n",
    "    # Encoder layers\n",
    "    encoder_output = embeddings\n",
    "    for i in range(config.NUM_LAYERS):\n",
    "        encoder_output = attention(encoder_output, encoder_output, encoder_output, i)\n",
    "\n",
    "    print(encoder_output.shape)\n",
    "    # MLM prediction head\n",
    "    mlm_output = layers.Dense(config.VOCAB_SIZE, activation=\"softmax\", name=\"mlm_cls\")(encoder_output)\n",
    "\n",
    "    # Build and compile the model\n",
    "    print(mlm_output.shape)\n",
    "\n",
    "    mlm_model = MaskedLanguageModel(inputs=inputs, outputs=mlm_output, name=\"masked_bert_model\")\n",
    "\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=config.LR)\n",
    "    mlm_model.compile(optimizer=optimizer)\n",
    "    \n",
    "    return mlm_model\n",
    "\n",
    "bert_mlm = build_bert_encoder()\n",
    "bert_mlm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "3592a4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2token = dict(enumerate(vectorize_layer.get_vocabulary()))\n",
    "token2id = {y: x for x, y in id2token.items()}\n",
    "\n",
    "class MaskedTextGenerator(keras.callbacks.Callback):\n",
    "    def __init__(self, sample_tokens, mask_token_id, top_k=5):\n",
    "        self.sample_tokens = sample_tokens\n",
    "        self.mask_token_id = mask_token_id\n",
    "        self.k = top_k\n",
    "\n",
    "    def decode(self, tokens):\n",
    "        \"\"\"Converts token IDs back to text. Converts tensor to a list first.\"\"\"\n",
    "        if isinstance(tokens, tf.Tensor):\n",
    "            tokens = tokens.numpy()  # Convert tensor to NumPy array\n",
    "        elif isinstance(tokens, np.ndarray):\n",
    "            tokens = tokens.tolist()  # Convert NumPy array to list\n",
    "        return \" \".join([id2token[t] for t in tokens if t != 0])\n",
    "\n",
    "    def convert_ids_to_tokens(self, id):\n",
    "        \"\"\"Converts a single token ID to the corresponding token.\"\"\"\n",
    "        return id2token.get(id, \"[UNK]\")\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        \"\"\"Generates and prints predictions for the masked tokens at the end of each epoch.\"\"\"\n",
    "        print(f\"Epoch {epoch + 1} ended. Generating predictions...\")\n",
    "\n",
    "        # Make predictions on the sample tokens\n",
    "        prediction = self.model.predict(self.sample_tokens)\n",
    "        print(f\"Prediction shape: {prediction.shape}\")\n",
    "\n",
    "        # Find the indices of masked tokens\n",
    "        masked_indices = np.where(self.sample_tokens == self.mask_token_id)\n",
    "        if len(masked_indices[0]) == 0:\n",
    "            print(\"No masked tokens found in the sample.\")\n",
    "            return\n",
    "\n",
    "        masked_indices = masked_indices[1]\n",
    "        print(f\"Masked token indices: {masked_indices}\")\n",
    "\n",
    "        # Make a copy of the original sample tokens to update incrementally\n",
    "        tokens = np.copy(self.sample_tokens[0])\n",
    "\n",
    "        # Process each masked token and its predictions\n",
    "        for idx in masked_indices:\n",
    "            mask_prediction = prediction[0][idx]\n",
    "\n",
    "            # Get the top-k predictions\n",
    "            top_indices = mask_prediction.argsort()[-self.k:][::-1]\n",
    "            top_probabilities = mask_prediction[top_indices]\n",
    "\n",
    "            # Iterate over the top-k predictions\n",
    "            for rank, (predicted_id, probability) in enumerate(zip(top_indices, top_probabilities), start=1):\n",
    "                # Update the tokens incrementally for each masked index\n",
    "                tokens[idx] = predicted_id\n",
    "\n",
    "                result = {\n",
    "                    \"epoch\": epoch + 1,\n",
    "                    \"input_text\": self.decode(self.sample_tokens[0]),\n",
    "                    \"predicted_text\": self.decode(tokens),\n",
    "                    \"probability\": probability,\n",
    "                    \"predicted_token\": self.convert_ids_to_tokens(predicted_id),\n",
    "                    \"rank\": rank\n",
    "                }\n",
    "                pprint(result)\n",
    "\n",
    "# # Initialize sample tokens and the callback\n",
    "sample_text = [\"Definitely a [mask] buy. I [mask] recommend it for anyone [mask] for quality and reliability.\"]\n",
    "sample_tokens = encode(sample_text, vectorize_layer)\n",
    "\n",
    "generator_callback = MaskedTextGenerator(sample_tokens, mask_token_id)\n",
    "\n",
    "\n",
    "# sample_tokens = vectorizer([\"Definitely a good buy. I strongly [mask] it\"])\n",
    "# generator_callback = MaskedTextGenerator(sample_tokens.numpy())\n",
    "# sample_tokens2 = vectorizer([\"Definitely a [mask] buy. I strongly recommend it\"])\n",
    "# generator_callback2 = MaskedTextGenerator(sample_tokens2.numpy())\n",
    "# sample_tokens3 = vectorizer([\"Definitely a good [mask]. I strongly recommend it\"])\n",
    "# generator_callback3 = MaskedTextGenerator(sample_tokens3.numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "f583d071",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-08 12:52:14.473059: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 408/3125 [==>...........................] - ETA: 12:04 - loss: 0.7648Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/esraonal/miniconda3/envs/tensorflow2/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3508, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/_7/cxy2r8fn70774bt9rncztq580000gn/T/ipykernel_12290/939367481.py\", line 1, in <module>\n",
      "    bert_mlm.fit(mlm_ds, epochs=20, callbacks=[generator_callback])\n",
      "  File \"/Users/esraonal/miniconda3/envs/tensorflow2/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/Users/esraonal/miniconda3/envs/tensorflow2/lib/python3.8/site-packages/keras/engine/training.py\", line 1409, in fit\n",
      "    tmp_logs = self.train_function(iterator)\n",
      "  File \"/Users/esraonal/miniconda3/envs/tensorflow2/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py\", line 150, in error_handler\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/Users/esraonal/miniconda3/envs/tensorflow2/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\", line 915, in __call__\n",
      "    result = self._call(*args, **kwds)\n",
      "  File \"/Users/esraonal/miniconda3/envs/tensorflow2/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\", line 947, in _call\n",
      "    return self._stateless_fn(*args, **kwds)  # pylint: disable=not-callable\n",
      "  File \"/Users/esraonal/miniconda3/envs/tensorflow2/lib/python3.8/site-packages/tensorflow/python/eager/function.py\", line 2453, in __call__\n",
      "    return graph_function._call_flat(\n",
      "  File \"/Users/esraonal/miniconda3/envs/tensorflow2/lib/python3.8/site-packages/tensorflow/python/eager/function.py\", line 1860, in _call_flat\n",
      "    return self._build_call_outputs(self._inference_function.call(\n",
      "  File \"/Users/esraonal/miniconda3/envs/tensorflow2/lib/python3.8/site-packages/tensorflow/python/eager/function.py\", line 497, in call\n",
      "    outputs = execute.execute(\n",
      "  File \"/Users/esraonal/miniconda3/envs/tensorflow2/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\", line 54, in quick_execute\n",
      "    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/esraonal/miniconda3/envs/tensorflow2/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 2105, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "  File \"/Users/esraonal/miniconda3/envs/tensorflow2/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 1396, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "  File \"/Users/esraonal/miniconda3/envs/tensorflow2/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 1287, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "  File \"/Users/esraonal/miniconda3/envs/tensorflow2/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 1140, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "  File \"/Users/esraonal/miniconda3/envs/tensorflow2/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 1055, in format_exception_as_a_whole\n",
      "    frames.append(self.format_record(record))\n",
      "  File \"/Users/esraonal/miniconda3/envs/tensorflow2/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 955, in format_record\n",
      "    frame_info.lines, Colors, self.has_colors, lvals\n",
      "  File \"/Users/esraonal/miniconda3/envs/tensorflow2/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 778, in lines\n",
      "    return self._sd.lines\n",
      "  File \"/Users/esraonal/miniconda3/envs/tensorflow2/lib/python3.8/site-packages/stack_data/utils.py\", line 144, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/Users/esraonal/miniconda3/envs/tensorflow2/lib/python3.8/site-packages/stack_data/core.py\", line 734, in lines\n",
      "    pieces = self.included_pieces\n",
      "  File \"/Users/esraonal/miniconda3/envs/tensorflow2/lib/python3.8/site-packages/stack_data/utils.py\", line 144, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/Users/esraonal/miniconda3/envs/tensorflow2/lib/python3.8/site-packages/stack_data/core.py\", line 677, in included_pieces\n",
      "    scope_pieces = self.scope_pieces\n",
      "  File \"/Users/esraonal/miniconda3/envs/tensorflow2/lib/python3.8/site-packages/stack_data/utils.py\", line 144, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/Users/esraonal/miniconda3/envs/tensorflow2/lib/python3.8/site-packages/stack_data/core.py\", line 614, in scope_pieces\n",
      "    scope_start, scope_end = self.source.line_range(self.scope)\n",
      "  File \"/Users/esraonal/miniconda3/envs/tensorflow2/lib/python3.8/site-packages/stack_data/core.py\", line 178, in line_range\n",
      "    return line_range(self.asttext(), node)\n",
      "  File \"/Users/esraonal/miniconda3/envs/tensorflow2/lib/python3.8/site-packages/executing/executing.py\", line 333, in asttext\n",
      "    self._asttext = ASTText(self.text, tree=self.tree, filename=self.filename)\n",
      "  File \"/Users/esraonal/miniconda3/envs/tensorflow2/lib/python3.8/site-packages/asttokens/asttokens.py\", line 305, in __init__\n",
      "    super(ASTText, self).__init__(source_text, filename)\n",
      "  File \"/Users/esraonal/miniconda3/envs/tensorflow2/lib/python3.8/site-packages/asttokens/asttokens.py\", line 47, in __init__\n",
      "    source_text = six.ensure_text(source_text)\n",
      "AttributeError: module 'six' has no attribute 'ensure_text'\n"
     ]
    }
   ],
   "source": [
    "bert_mlm.fit(mlm_ds, epochs=20, callbacks=[generator_callback])\n",
    "bert_mlm.save(\"bert_mlm_amazon_sep_epoch_20.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c318865-fff3-4491-9f2a-6134d9df1ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_bert_lstm_encoder():\n",
    "    \"\"\"\n",
    "    Constructs a BERT-like masked language model encoder.\n",
    "\n",
    "    Args:\n",
    "        config: A configuration object containing attributes like MAX_LEN, VOCAB_SIZE, EMBED_DIM, NUM_LAYERS, LR.\n",
    "\n",
    "    Returns:\n",
    "        A compiled TensorFlow model ready for training.\n",
    "    \"\"\"\n",
    "    # Define the input layer\n",
    "    inputs = layers.Input(shape=(config.MAX_LEN,), dtype=tf.int64, name=\"input_ids\")\n",
    "    \n",
    "    # Embedding layers: token embeddings + positional embeddings\n",
    "    word_embeddings = layers.Embedding(\n",
    "        input_dim=config.VOCAB_SIZE, output_dim=config.EMBED_DIM, name=\"word_embedding\"\n",
    "    )(inputs)\n",
    "    \n",
    "    position_embeddings = PositionEmbeddingLayer(\n",
    "        sequence_length=config.MAX_LEN, output_dim=config.EMBED_DIM, name=\"position_embedding\"\n",
    "    )(inputs)\n",
    "\n",
    "    # Sum embeddings\n",
    "    embeddings = layers.Add(name=\"embeddings_add\")([word_embeddings, position_embeddings])\n",
    "\n",
    "    # Encoder layers\n",
    "    encoder_output = embeddings\n",
    "    for i in range(config.NUM_LAYERS):\n",
    "        encoder_output = attention(encoder_output, encoder_output, encoder_output, i)\n",
    "\n",
    "    # LSTM layers taking word embeddings directly\n",
    "    lstm_output = layers.LSTM(config.EMBED_DIM, return_sequences=True)(word_embeddings)\n",
    "    lstm_output = layers.Dropout(0.1)(lstm_output)\n",
    "    lstm_output = layers.LSTM(config.EMBED_DIM, return_sequences=True)(lstm_output)\n",
    "    lstm_output = layers.Dropout(0.1)(lstm_output)\n",
    "\n",
    "    # Encoder output and LSTM output are added together\n",
    "    lstm_attention = encoder_output + lstm_output\n",
    "\n",
    "    # MLM prediction head\n",
    "    mlm_output = layers.Dense(config.VOCAB_SIZE, activation=\"softmax\", name=\"mlm_cls\")(encoder_output)\n",
    "\n",
    "    # Build and compile the model\n",
    "    print(mlm_output.shape)\n",
    "\n",
    "    mlm_model = MaskedLanguageModel(inputs=inputs, outputs=mlm_output, name=\"masked_bert_lstm_model\")\n",
    "\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=config.LR)\n",
    "    mlm_model.compile(optimizer=optimizer)\n",
    "    \n",
    "    return mlm_model\n",
    "\n",
    "bert_lstm_mlm = build_bert_lstm_encoder()\n",
    "bert_lstm_mlm.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eafd3f7-b3c8-4961-ba50-91c00d49011d",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_lstm_mlm.fit(mlm_ds, epochs=20, callbacks=[generator_callback])\n",
    "bert_lstm_mlm.save(\"bert_lstm_mlm_amazon_sep_epoch_20.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04748204-e052-490e-9f54-2c6074bb9b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lstm_encoder():\n",
    "    \"\"\"\n",
    "    Constructs a BERT-like masked language model encoder.\n",
    "\n",
    "    Args:\n",
    "        config: A configuration object containing attributes like MAX_LEN, VOCAB_SIZE, EMBED_DIM, NUM_LAYERS, LR.\n",
    "\n",
    "    Returns:\n",
    "        A compiled TensorFlow model ready for training.\n",
    "    \"\"\"\n",
    "    # Define the input layer\n",
    "    inputs = layers.Input(shape=(config.MAX_LEN,), dtype=tf.int64, name=\"input_ids\")\n",
    "    \n",
    "    word_embeddings = layers.Embedding(\n",
    "        input_dim=config.VOCAB_SIZE, output_dim=config.EMBED_DIM, name=\"word_embedding\"\n",
    "    )(inputs)\n",
    "    \n",
    "    # LSTM layers taking word embeddings directly\n",
    "    lstm_output = layers.LSTM(config.EMBED_DIM, return_sequences=True)(word_embeddings)\n",
    "    lstm_output = layers.Dropout(0.1)(lstm_output)\n",
    "    lstm_output = layers.LSTM(config.EMBED_DIM, return_sequences=True)(lstm_output)\n",
    "    lstm_output = layers.Dropout(0.1)(lstm_output)\n",
    "\n",
    "    # MLM prediction head\n",
    "    lstm_output = layers.Dense(config.VOCAB_SIZE, activation=\"softmax\", name=\"mlm_cls\")(lstm_output)\n",
    "\n",
    "    mlm_lstm_model = MaskedLanguageModel(inputs=inputs, outputs=lstm_output, name=\"masked_bert_lstm_model\")\n",
    "\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=config.LR)\n",
    "    mlm_lstm_model.compile(optimizer=optimizer)\n",
    "    \n",
    "    return mlm_lstm_model\n",
    "\n",
    "lstm_mlm = build_lstm_encoder()\n",
    "lstm_mlm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66cd4490-a26a-4832-9212-251fdf0037bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_mlm.fit(mlm_ds, epochs=20, callbacks=[generator_callback])\n",
    "lstm_mlm.save(\"lstm_mlm_amazon_sep_epoch_20.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104c3cf3-44ef-462d-aa76-f2ee70b6abfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Suppress TensorFlow logging\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "def decode(tokens):\n",
    "    \"\"\"Converts a sequence of token IDs into a text string.\"\"\"\n",
    "    return \" \".join([id2token.get(t, \"[UNK]\") for t in tokens if t != 0])\n",
    "\n",
    "def convert_ids_to_tokens(token_id):\n",
    "    \"\"\"Converts a single token ID to its corresponding token.\"\"\"\n",
    "    return id2token.get(token_id, \"[UNK]\")\n",
    "\n",
    "def generate_next_word(text, max_length, model):\n",
    "    \"\"\"\n",
    "    Generates the next word in the sequence for a given input text, up to max_length.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input text to which words will be added.\n",
    "        max_length (int): Maximum number of words to generate.\n",
    "        model (tf.Model): Trained language model used to predict the next word.\n",
    "\n",
    "    Returns:\n",
    "        str: The generated sequence after appending predicted words.\n",
    "    \"\"\"\n",
    "    for _ in range(max_length):\n",
    "        # Append [mask] token to the text to predict the next word\n",
    "        predict_input = text + \" [mask]\"\n",
    "        sample_tokens = encode(predict_input, vectorize_layer)\n",
    "\n",
    "        # Predict masked word\n",
    "        prediction = model.predict(sample_tokens.numpy(), verbose=0)\n",
    "\n",
    "        # Find the index of the [mask] token in the sample\n",
    "        masked_index = np.where(sample_tokens == mask_token_id)\n",
    "        if len(masked_index[1]) == 0:\n",
    "            print(\"No masked tokens found.\")\n",
    "            return text\n",
    "        \n",
    "        masked_index = masked_index[1][0]  # Get first masked index\n",
    "\n",
    "        # Get the prediction for the masked token\n",
    "        mask_prediction = prediction[0][masked_index]\n",
    "\n",
    "        # Get the top prediction\n",
    "        top_index = mask_prediction.argsort()[-1:][::-1][0]  # Get top prediction index\n",
    "        predicted_token = convert_ids_to_tokens(top_index)\n",
    "\n",
    "        # Add the predicted word to the text and continue\n",
    "        text = text + \" \" + predicted_token\n",
    "\n",
    "    return text\n",
    "\n",
    "# Example usage\n",
    "text = \"Absolutely amazing\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db569632-3539-48a9-b020-fe0aa61ba7be",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_text1 = generate_next_word(text, 25, bert_mlm)\n",
    "generated_text2 = generate_next_word(text, 25, lstm_mlm)\n",
    "generated_text3 = generate_next_word(text, 25, bert_lstm_mlm)\n",
    "\n",
    "print(f\"Final generated text from BERT: {generated_text1}\")\n",
    "print(f\"Final generated text from LSTM: {generated_text2}\")\n",
    "print(f\"Final generated text from BERT_LSTM: {generated_text3}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9(tensorflow2)",
   "language": "python",
   "name": "tensorflow2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
